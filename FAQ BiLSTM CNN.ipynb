{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM and CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model to classify user generated content (UGC) as FAQ or not FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Shreyash Gupta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organization: IndiaMART InterMESH Pvt. Ltd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the embeddings into a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2838it [00:00, 8308.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectors:  2838\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "vec = codecs.open('faqtrain.vec', encoding='utf-8')\n",
    "for line in tqdm(vec):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "vec.close()\n",
    "print(\"Word vectors: \", len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating data frames for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_train = pd.read_excel(\"faqtrain.xlsx\")\n",
    "faq_test = pd.read_excel(\"faqtest.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(424, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length =  12\n"
     ]
    }
   ],
   "source": [
    "faq_train[\"Doc Length\"] = faq_train[\"Question Title\"].apply(lambda words: len(words.split(\" \")))\n",
    "MAX_SEQ_LEN = np.round(faq_train[\"Doc Length\"].mean() + faq_train[\"Doc Length\"].std()).astype(int)\n",
    "print(\"Max sequence length = \", MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading train and test lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 1500000\n",
    "label_names = ['Remarks']\n",
    "y_train = faq_train[label_names].values\n",
    "processed_docs_train = faq_train['Question Title'].tolist()\n",
    "processed_docs_test = faq_test['Question Title'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size:  13024\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS, lower = True, char_level = False)\n",
    "tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Dictionary size: \", len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding sequences to same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen = MAX_SEQ_LEN)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen = MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing embedding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null word embeddings:  11893\n",
      "Words not found:  11892\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of null word embeddings: \",np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"Words not found: \", len(words_not_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the core model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Conv1D, MaxPooling1D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 512\n",
    "weight_decay = 1e-4\n",
    "num_classes = len(label_names)\n",
    "batch_size = 1024\n",
    "num_epochs = 5\n",
    "embed_dim = 300 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0618 15:32:15.135451  3272 deprecation.py:506] From C:\\Users\\Pooja\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embed_dim, weights = [embedding_matrix], input_length = MAX_SEQ_LEN, trainable=False))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences = True, dropout = 0.25, recurrent_dropout = 0.1)))\n",
    "model.add(Conv1D(num_filters, 5, activation = 'relu', padding = 'same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation = 'relu', padding = 'same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation = 'relu', kernel_regularizer = regularizers.l2(weight_decay)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "model.compile(loss  ='binary_crossentropy', optimizer = adam, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 12, 300)           3907500   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 12, 200)           320800    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 12, 512)           512512    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 6, 512)            1835520   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 3, 512)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 512)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3, 32)             16416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 6,592,845\n",
      "Trainable params: 2,685,345\n",
      "Non-trainable params: 3,907,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.factorize(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18258 samples, validate on 2029 samples\n",
      "Epoch 1/5\n",
      "18258/18258 - 87s - loss: 0.3575 - accuracy: 0.8343 - val_loss: 0.0646 - val_accuracy: 0.9832\n",
      "Epoch 2/5\n",
      "18258/18258 - 84s - loss: 0.1768 - accuracy: 0.9306 - val_loss: 0.0539 - val_accuracy: 0.9857\n",
      "Epoch 3/5\n",
      "18258/18258 - 87s - loss: 0.1461 - accuracy: 0.9441 - val_loss: 0.0503 - val_accuracy: 0.9862\n",
      "Epoch 4/5\n",
      "18258/18258 - 91s - loss: 0.1327 - accuracy: 0.9516 - val_loss: 0.0504 - val_accuracy: 0.9872\n",
      "Epoch 5/5\n",
      "18258/18258 - 86s - loss: 0.1248 - accuracy: 0.9534 - val_loss: 0.0481 - val_accuracy: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13e59e50f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_seq_train, y[0], batch_size = batch_size, epochs = num_epochs, validation_split = 0.1, shuffle = True, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = faq_test[label_names].values\n",
    "y_test = y_test.reshape(-1,)\n",
    "y_t = pd.factorize(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424/424 [==============================] - 0s 403us/sample - loss: 2.5613 - accuracy: 0.1250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5613081455230713, 0.125]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(word_seq_test, y_t[0], batch_size = batch_size)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20287/20287 [==============================] - ETA: 6s - loss: 0.3726 - accuracy: 0.85 - ETA: 6s - loss: 0.2263 - accuracy: 0.91 - ETA: 5s - loss: 0.1818 - accuracy: 0.93 - ETA: 5s - loss: 0.1379 - accuracy: 0.94 - ETA: 5s - loss: 0.1325 - accuracy: 0.95 - ETA: 4s - loss: 0.1550 - accuracy: 0.94 - ETA: 4s - loss: 0.1497 - accuracy: 0.94 - ETA: 4s - loss: 0.1383 - accuracy: 0.94 - ETA: 3s - loss: 0.1343 - accuracy: 0.95 - ETA: 3s - loss: 0.1248 - accuracy: 0.95 - ETA: 3s - loss: 0.1335 - accuracy: 0.95 - ETA: 2s - loss: 0.1360 - accuracy: 0.94 - ETA: 2s - loss: 0.1287 - accuracy: 0.95 - ETA: 2s - loss: 0.1197 - accuracy: 0.95 - ETA: 1s - loss: 0.1120 - accuracy: 0.95 - ETA: 1s - loss: 0.1080 - accuracy: 0.96 - ETA: 0s - loss: 0.1019 - accuracy: 0.96 - ETA: 0s - loss: 0.1001 - accuracy: 0.96 - ETA: 0s - loss: 0.0957 - accuracy: 0.96 - 7s 340us/sample - loss: 0.0958 - accuracy: 0.9653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09576678113549963, 0.9653473]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(word_seq_train, y[0], batch_size = batch_size)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(word_seq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(columns = ['QT'] + label_names)\n",
    "submission_df['QT'] = faq_test['Question Title'].values \n",
    "submission_df[label_names] = y_pred \n",
    "submission_df.to_excel(\"faq_BiLSTM_CNN_pred.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
