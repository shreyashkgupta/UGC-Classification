{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning of FAQ Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning of Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Shreyash Gupta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organization: IndiaMART InterMESH Pvt. Ltd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_train = pd.read_excel(\"faq.xlsx\")\n",
    "faq_test = pd.read_excel(\"faq_test.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_train['Remarks'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_test['Remarks'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_train['cleanQT'] = faq_train['Question Title'].str.replace(\"[0-9]\",\" \")\n",
    "faq_test['cleanQT'] = faq_test['Question Title'].str.replace(\"[0-9]\",\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove special symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_symbols = '!@#$%^&*()_-+=[]\\{}|;\",.<>/?~:\\\"'\n",
    "faq_train['cleanQT'] = faq_train['cleanQT'].apply(lambda rss: ''.join(ch for ch in rss if ch not in set(special_symbols)))\n",
    "faq_test['cleanQT'] = faq_test['cleanQT'].apply(lambda rss: ''.join(ch for ch in rss if ch not in set(special_symbols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Convert all characters to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_train['cleanQT'] = faq_train['cleanQT'].str.lower()\n",
    "faq_test['cleanQT'] = faq_test['cleanQT'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Remove white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_test['cleanQT'] = faq_test['cleanQT'].apply(lambda rws: ' '.join(rws.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing differences before/after preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data\")\n",
    "print(faq_train.sample(5))\n",
    "print(\"Testing data\")\n",
    "print(faq_test.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing ELMo vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the ELMo module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining function for creating ELMo vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(text):\n",
    "    embeddings = elmo(text.tolist(),signature = \"default\", as_dict = True)[\"elmo\"]\n",
    "    with tf.compat.v1.Session() as session:\n",
    "        session.run(tf.compat.v1.global_variables_initializer())\n",
    "        session.run(tf.compat.v1.tables_initializer())\n",
    "        return session.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dataset into batches for better computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_start_time = time.time()\n",
    "faq_train_list = [faq_train[i:i+100] for i in range(0,faq_train.shape[0],100)]\n",
    "faq_test_list = [faq_test[i:i+100] for i in range(0,faq_test.shape[0],100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting ELMo vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_extraction_start_time = time.time()\n",
    "faq_elmo_train = [elmo_vectors(x['cleanQT']) for x in faq_train_list]\n",
    "faq_elmo_test = [elmo_vectors(x['cleanQT']) for x in faq_test_list]\n",
    "elmo_extraction_end_time = time.time()\n",
    "print(\"Total extraction time for ELMo vectors: {} seconds\".format(elmo_extraction_end_time - elmo_extraction_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenatening all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_concat_start_time = time.time()\n",
    "elmo_faq_train = np.concatenate(faq_elmo_train, axis = 0)\n",
    "elmo_faq_test = np.concatenate(faq_elmo_test, axis = 0)\n",
    "elmo_end_time = elmo_concat_end_time = time.time()\n",
    "print(\"Total concatenation time: {} seconds\".format(elmo_concat_end_time - elmo_concat_start_time))\n",
    "print(\"Total time for ELMo vector extraction: {} seconds\".format(elmo_end_time - elmo_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving output to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out_train = open(\"elmo_faq_train_04062019.pickle\",\"wb\")\n",
    "pickle_out_test = open(\"elmo_faq_test_04062019.pickle\",\"wb\")\n",
    "pickle.dump(elmo_faq_train, pickle_out_train)\n",
    "pickle.dump(elmo_faq_test,pickle_out_test)\n",
    "pickle_out_train.close()\n",
    "pickle_out_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading ELMo vectors pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in_train = open(\"elmo_faq_train_04062019.pickle\",\"rb\")\n",
    "pickle_in_test = open(\"elmo_faq_test_04062019.pickle\",\"rb\")\n",
    "elmo_faq_train = pickle.load(pickle_in_train)\n",
    "elmo_faq_test = pickle.load(pickle_in_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.DataFrame(elmo_faq_train)\n",
    "ytrain = faq_train['Remarks']\n",
    "xvalid = pd.DataFrame(elmo_faq_test)\n",
    "yvalid = faq_test['Remarks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buidling a logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_start_time = time.time()\n",
    "regressor = LogisticRegression()\n",
    "regressor.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_val_lr = regressor.predict(xvalid)\n",
    "lr_end_time = time.time()\n",
    "print(\"Total time spent on LR: {} seconds\".format(lr_end_time - lr_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression\")\n",
    "print(\"Precision: \",precision_score(yvalid,pred_val_lr,pos_label='FAQ'))\n",
    "print(\"Recall: \",recall_score(yvalid,pred_val_lr,pos_label='FAQ'))\n",
    "print(\"F1 Score: \",f1_score(yvalid, pred_val_lr,pos_label='FAQ'))\n",
    "print(\"MCC: \",matthews_corrcoef(yvalid, pred_val_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = ['l1','l2']\n",
    "t_list = [0.00001, 0.0001, 0.001, 0.01]\n",
    "c_list = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "fi_list = [True, False]\n",
    "cw_list = [None, 'balanced']\n",
    "mi_list = [50, 100, 150, 200]\n",
    "ws_list = [True, False]\n",
    "nj_list = [1, 2, 3, 4]\n",
    "pr = []\n",
    "r = []\n",
    "f1 = []\n",
    "mcc = []\n",
    "for nj in nj_list:\n",
    "    for ws in ws_list:\n",
    "        for mi in mi_list:\n",
    "            for cw in cw_list:\n",
    "                for fi in fi_list:\n",
    "                    for c in c_list:\n",
    "                        for t in t_list:\n",
    "                            for p in p_list:\n",
    "                                lr = LogisticRegression(penalty = p, tol = t, C = c, fit_intercept = fi, class_weight = cw, max_iter = mi, warm_start = ws, n_jobs = nj)\n",
    "                                lr.fit(xtrain, ytrain)\n",
    "                                pred_val_lr = lr.predict(xvalid)\n",
    "                                print(\"LR with p = {}, t = {}, c = {}, fi = {}, cw = {}, mi = {}, ws = {}, nj = {}\".format(p, t, c, fi, cw, mi, ws, nj))\n",
    "                                print(\"Precision: \",precision_score(yvalid,pred_val_lr,pos_label='FAQ'))\n",
    "                                pr.append(precision_score(yvalid,pred_val_lr,pos_label='FAQ'))\n",
    "                                print(\"Recall: \",recall_score(yvalid,pred_val_lr,pos_label='FAQ'))\n",
    "                                r.append(recall_score(yvalid,pred_val_lr,pos_label='FAQ'))\n",
    "                                print(\"F1 Score: \",f1_score(yvalid, pred_val_lr,pos_label='FAQ'))\n",
    "                                f1.append(f1_score(yvalid, pred_val_lr,pos_label='FAQ'))\n",
    "                                print(\"MCC: \",matthews_corrcoef(yvalid, pred_val_lr))\n",
    "                                mcc.append(matthews_corrcoef(yvalid, pred_val_lr))\n",
    "                                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max precision: \",max(pr))\n",
    "print(\"Max recall: \",max(r))\n",
    "print(\"Max F1: \",max(f1))\n",
    "print(\"Max MCC: \",max(mcc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
